- 概述
  collapsed:: true
	- （数据收集）
	  collapsed:: true
		- 基于历史数据
		- 观测性研究
		- 实验设计
	- 回归分析
	  collapsed:: true
		- 通过一组预测变量，来预测一个（有时为多个）响应变量的统计方法；也可用于评估预测变量对响应变量的影响，
	- 样本
	  collapsed:: true
		- 一般将数据分为两组，一组称为预测变量X，另一组记为响应变量Y；样本量一般用 n 表示，
		- 理论上X，Y都为*随机变量*，但为了便于分析，一般将*X看成常量*，
	- 模型
	  collapsed:: true
		- 最基本的模型为$Y = \beta_{0} + \beta_{1}X + \varepsilon$，
		- 广义的多元模型为$Y = \beta_{0} + \beta_{1}f_{1}(u) + \beta_{2}f_{2}(v) + \varepsilon$，
		- 线性
		  collapsed:: true
			- 指模型中的参数βi为线性（一次），并非因变量Y为自变量X的线性函数，
		- 多元
		  collapsed:: true
			- 若因变量Y与多个自变量Xi都有关系，则称模型为多元线性回归模型，
	- @模型的选择
	  collapsed:: true
		- 建立模型的步骤
		- 模型选择标准
		- 判断模型的可行性
	- 模型的应用
	  collapsed:: true
		- 描述变量间的关系，
		  collapsed:: true
			- 变量间的相关性不能说明变量间的因果性，
		- 控制变量X的变化以影响Y的取值，
		- 预测
		  collapsed:: true
			- 由于数据中的X一般为离散的多个值，而模型一般为连续的函数，因此可以用于预测不在给定数据中的值，
			- 然而，模型在“外推”时（即新给定的预测变量不在原来数据的最大值最小值范围内），其预测的准确性可能会下降，
-
- 一元线性回归（SLR）
	- 标准一元回归模型
	  collapsed:: true
		- 模型与假定
		  collapsed:: true
			- 模型：$Y_{i} = \beta_{0} + \beta_{1}X_{i} + \varepsilon_{i}$，
			  collapsed:: true
				- β0和β1为未知参数（常量），
				  collapsed:: true
					- β0为模型的截距（intercept），
					- β1为模型的斜率（slope），
				- 将Xi看成确定的值（常量），称为*预测*变量，
				- εi为随机变量，且两两不相关，
				- 因此Yi也为随机变量，称为*响应*变量，
				  collapsed:: true
					- 为了保留X的随机性，一般用条件概率标记，
					- EY = E(Y|x) = β0 + β1X,
					- Var Y = Var (Y|x) = σ2，
			- （方差）统计量
			  collapsed:: true
				- 根据一元回归模型的假定，只有Yi为随机变量；因此Yi的函数也为随机变量，
				- $S_{XX} = \sum\limits_{i = 1}^{n}(X_{i} - \bar{X})^{2} = \sum\limits_{i = 1}^{n}X_i^{2} - n\bar{X}^{2}$，
				- $S_{XY} = \sum\limits_{i = 1}^{n}(X_{i} - \bar{X})(Y_{i} - \bar{Y}) = \sum\limits_{i = 1}^{n}X_{i}Y_{i} - n\bar{X}\bar{Y}$，
					- $= \sum\limits_{i = 1}^{n}X_{i}Y_{i} - \bar{X}\sum\limits_{i = 1}^{n}Y_{i}$，
					- $= \sum\limits_{i = 1}^{n}(X_{i} - \bar{X})Y_{i}$，即可以表示为Yi的线性组合的形式，
					- 一般记$c_{i} = \dfrac{X_{i} - \bar{X}}{S_{XX}}$，
				- $S_{YY} = \sum\limits_{i = 1}^{n}(Y_{i} - \bar{Y})^{2} = \sum\limits_{i = 1}^{n}Y_i^{2} - n\bar{Y}^{2}$，
			- 统计假定
			  collapsed:: true
				- 一般记$\varepsilon_{i} \sim N(0, \sigma^{2})$，
				- 根据正态分布的性质，可知$Y_{i} \sim N(\beta_{0} + \beta_{1}X_{i}, \sigma^{2})$，
				  collapsed:: true
					- 即Y_{i}独立同方差，但均值不相同，
		- 模型的统计推断
		  collapsed:: true
			- 参数的点估计
				- 最小二乘法（least square）
				  collapsed:: true
					- 模型
					  collapsed:: true
						- 设实际参数为b_{0}，b_{1}，
						- 定义距离$D = Y_{i} - b_{0} - b_{1}X_{i}$，
						- 定义距离的“总和”$Q = \sum\limits_{i = 1}^{n}(Y_{i} - b_{0} - b_{1}X_{i})^{2}$，
						  collapsed:: true
							- 由于距离可能有正有负，所以此处选用距离的平方和，
						- 称使Q最小的b_{0}，b_{1}为模型的最小二乘解，
					- 参数求解
					  collapsed:: true
						- 最小二乘的求解没有考虑统计假定，而是利用*微积分*方式求解，
						- 设*函数*$Q(b_{0}, b_{1}) = \sum\limits_{i = 1}^{n}(Y_{i} - b_{0} - b_{1}X_{i})^{2}$，此处Xi，Yi为确定的样本值 ，
						  id:: 63bbca20-3577-4c0b-a107-6c1c712ad364
						- 求解偏导数
						  collapsed:: true
							- $\dfrac{\partial Q}{\partial b_{0}} = -2\sum\limits_{i = 1}^{n}(Y_{i} - b_{0} - b_{1}X_{i})$，
							- $\dfrac{\partial Q}{\partial b_{1}} = -2X_{i}\sum\limits_{i = 1}^{n}(Y_{i} - b_{0} - b_{1}X_{i})$，
						- 令偏导数为0
						  collapsed:: true
							- $\hat{b_{0}} = \bar{Y} - \hat{b_{1}}\bar{X}$
							- $\hat{b_{1}} = \dfrac{\sum\limits_{i = 1}^{n}X_{i}Y_{i} - n\bar{X}\bar{Y}}{\sum\limits_{i = 1}^{n}X_{i}^{2} - n\bar{X}^{2}} = \dfrac{S_{XY}}{S_{XX}}$，
				- 最大似然估计（MLE）
				  collapsed:: true
					- 模型
					  collapsed:: true
						- 由于Yi为正态随机变量，β_{0}，β_{1}为Yi的参数，
						- 即$Y_{i} \sim N(\beta_{0} + \beta_{1}X_{i}, \sigma^{2})$，
						- 所以可以求解似然函数， 通过求解似然函数的极值获得β_{0}，β_{1}的最大似然估计，
					- 参数求解
					  collapsed:: true
						- $f_{Y}(y_{i})=\frac{1}{\sqrt{2 \pi} \sigma} \exp{(-\frac{(Y_{i} - \beta_{0} - \beta_{1}X_{i})^{2}}{2 \sigma^{2}})}$，
						- $L(\beta_{0}, \beta_{1}) = (2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}\exp[{(-\frac{\sum\limits_{i = 1}^{n}(Y_{i} - \beta_{0} - \beta_{1}X_{i})^{2}}{2 \sigma^{2}})}]$，
						- 从似然函数的表达式可以看出，MLE的形式与最小二乘的形式相同，
					- 估计量
					  collapsed:: true
						- 此处X看成常量，Y看成随机变量，
						- b0，b1作为*Y的函数*，也为随机变量，
						- $\hat{b_{1}} = \dfrac{\sum\limits_{i = 1}^{n}X_{i}Y_{i} - n\bar{X}\bar{Y}}{\sum\limits_{i = 1}^{n}X_{i}^{2} - n\bar{X}^{2}} = \dfrac{S_{XY}}{S_{XX}} = \sum\limits_{i = 1}^{n}c_{i}Y_{i}$，
						- $\hat{b_{0}} = \bar{Y} - \hat{b_{1}}\bar{X} = \dfrac{1}{n}\sum\limits_{i = 1}^{n}Y_{i} - \bar{X}\sum\limits_{i = 1}^{n}c_{i}Y_{i} = \sum\limits_{i = 1}^{n}(\dfrac{1}{n} - \bar{X}c_{i})Y_{i}$
				- 最佳线性无偏估计（BLUE）
				- 估计量的性质
				  collapsed:: true
					- 没有正态性假定时，估计量b0，b1也具有下述性质，
					- 无偏性
					  collapsed:: true
						- E(b0) = β0，E(b1) = β1，
						- 也称为Gauss-Markov定理，
					- 方差
						- b0和b1为最小方差无偏估计，
						- $\text{Var} (b_{1}) = \sum \limits _{i= 1}^{n}c_{i}^{2}\text{Var}{Y_{i}} = \dfrac{\sigma^{2}}{S_{XX}}$，
						- $\text{Var} (b_{0}) = \text{Var}[\bar{y} - b_{1}\bar{x}] = \text{Var}(\bar{y}) + {\bar{x}}^{2}\text{Var}(b_{1}) = \dfrac{\sigma^{2}}{n} + {\bar{x}}^{2}\dfrac{\sigma^{2}}{S_{XX}}$，
			- 方差的估计
				- 定义
				  collapsed:: true
					- 称$e_{i} = Y_{i} - \hat{Y_{i}}$为*残差*；其中$Y_{i}$为样本值，$\hat{Y_{i}}$为估计值（拟合值），
					- 相比之下，模型的*误差*$\varepsilon_{i} = Y_{i} - EY_{i}$，
					- 此处残差是可以计算得到的量，而误差则是不可测的随机变量，
					- 对于模型$Y_{i} \sim N(\beta_{0} + \beta_{1}X_{i}, \sigma^{2})$，一般用残差来估计方差，
				- 性质
				  collapsed:: true
					- $\sum\limits_{i=1}^{n}e_{i} = 0$，
					  collapsed:: true
						- 求解最小二乘的偏导数时，有方程$-2\sum\limits_{i = 1}^{n}(Y_{i} - b_{0} - b_{1}X_{i}) = 0$，
					- 推论：$\sum\limits_{i=1}^{n}Y_{i} = \sum\limits_{i=1}^{n}\hat{Y_{i}}$，
					- $\sum\limits_{i=1}^{n}e_{i}^{2}$最小，
					  collapsed:: true
						- 最小二乘解的性质，
					- $\sum\limits_{i=1}^{n}X_{i}e_{i} = 0$，
					  collapsed:: true
						- 求解最小二乘的偏导数时，有方程$-2X_{i}\sum\limits_{i = 1}^{n}(Y_{i} - b_{0} - b_{1}X_{i}) = 0$，
					- 推论：$\sum\limits_{i=1}^{n}\hat{Y_{i}}e_{i} = 0$，
					  collapsed:: true
						- 可由$\sum\limits_{i=1}^{n}e_{i} = 0$，和$\sum\limits_{i=1}^{n}X_{i}e_{i} = 0$证明，
				- SSE（残差平方和）
				  collapsed:: true
					- 定义SSE = $\sum \limits _{i= 1}^{n}{e_{i}}^{2} = \sum \limits _{i= 1}^{n}(Y_{i} - \hat{Y_{i}})^{2}$，
					- 将$\hat{Y_{i}} = b_{0} + b_{1}X_{i}$带入SSE的公式，
					- 可算得SSE = $S_{YY} - b_{1}S_{XY}$，
				- MSE（残差均方和）
				  collapsed:: true
					- 定义MSE = $\hat{\sigma}^{2} = \dfrac{SSE}{n - 2}$，为方差的无偏估计，
					- 此处需要用Yi估计两个参数b_{0}，b_{1}，所以自由度为n - 2，
			- 参数的假设检验
			  collapsed:: true
				- 斜率β1的t检验
				  collapsed:: true
					- 假设
					  collapsed:: true
						- $H_{0} : \beta_{1} = \beta_{10} \leftrightarrow H_{1} : \beta_{1} \ne \beta_{10}$；此处β10为某个常数，
					- 检验统计量
					  collapsed:: true
						- b1为进行检验的随机变量，
						- $b_{1} \sim N(\beta_{10}, \dfrac{\sigma^{2}}{S_{XX}})$，
					- 拒绝域
					  collapsed:: true
						- 由于方差σ2*未知*，所以可构造t统计量，
						- 即$t = \dfrac{b1 - \beta_{10}}{\sqrt{MSE / S_{XX}}}$，
						  collapsed:: true
							- 其中，$(n - 2)MSE \sim \chi^{2}(n - 2)$，且与分子统计量b1独立，
							- 有时也将分母${\sqrt{MSE / S_{XX}}}$记为斜率的标准误差se(b1)，
						- H0成立的情况下，$t \sim t(n - 2)$，
						- 所以拒绝域为$|t_{0}| > t_{\frac{\alpha}{2}}(n - 2)$，
						-
				- 特例：显著性检验
				  collapsed:: true
					- 假设
					  collapsed:: true
						- $H_{0} : \beta_{1} = 0 \leftrightarrow H_{1} : \beta_{1} \ne 0$，
						- 由于β1为斜率，所以假设的含义为X，Y之间是否存在*线性*关系，
						- 然而，即使接受原假设，也不能说明线性模型为*最优*模型，
					- 可以直接应用β1的t检验，
					- 也可以使用方差分析方法，
				- 截距β0的t检验
				  collapsed:: true
					- 与b1的检验基本相同，
					- 假设
					  collapsed:: true
						- $H_{0} : \beta_{0} = \beta_{00} \leftrightarrow H_{1} : \beta_{0} \ne \beta_{00}$；此处β00为某个常数，
					- 检验统计量
					  collapsed:: true
						- b0为进行检验的随机变量，
						- $b_{0} \sim N(\beta_{00}, \dfrac{\sigma^{2}}{n} + {\bar{x}}^{2}\dfrac{\sigma^{2}}{S_{XX}})$，
					- 拒绝域
					  collapsed:: true
						- 由于方差σ2*未知*，所以可构造t统计量，
						- 即$t = \dfrac{b0 - \beta_{00}}{\sqrt{MSE(\frac{1}{n} + \frac{{\bar{x}}^{2}}{S_{XX}})}}$，
						  collapsed:: true
							- 其中，$(n - 2)MSE \sim \chi^{2}(n - 2)$，且与分子统计量b0独立，
							- 有时也将分母${\sqrt{MSE(\frac{1}{n} + \frac{{\bar{x}}^{2}}{S_{XX}})}}$记为斜率的标准误差se(b0)，
						- H0成立的情况下，$t \sim t(n - 2)$，
						- 所以拒绝域为$|t_{0}| > t_{\frac{\alpha}{2}}(n - 2)$，
						-
			- 参数的区间估计
				- b0和b1的区间估计
				  collapsed:: true
					- 根据假设检验的t统计量，可以反转拒绝域得到b0和b1的区间估计量，
				- σ2的区间估计
				  collapsed:: true
					- 根据定理$SSE = \dfrac{(n - 2)MSE}{\sigma^{2}} \sim \chi^{2}(n - 2)$，
					- 可以将SSE作为枢轴量，基于MSE构造σ2的置信区间，
			- 观测值的统计推断
				- 响应变量EYi
				  collapsed:: true
					- 概述
					  collapsed:: true
						- 根据模型假定，Yi为随机变量，因此是不可估计的，
						- 而取期望后的EYi为常量，可以看成参数β0和β1的函数，
					- 点估计
					  collapsed:: true
						- EYi的点估计为$b_{0} + b_{1}X_{i}$，一般记为$\hat{\mu}_{y|x_{i}}$，
						- 由于b0，b1都是随机变量Y的函数，因此$\hat{\mu}_{y|x_{i}}$也是随机变量Y的函数，
						- EYi的点估计可以直接由b0，b1算出，因此其实际意义不大，重要的为EYi的区间估计，
					- 区间估计
					  collapsed:: true
						- 分布
						  collapsed:: true
							- b0，b1都是随机变量Y的线性函数，
							- 所以$\hat{\mu}_{y|x_{i}}$也是随机变量Y的线性函数，因此也为正态分布，
							- @可以算得$\hat{\mu}_{y|x_{i}} \sim N(\beta_{0} + \beta_{1}X_{i}, \dfrac{\sigma^{2}}{n} + \dfrac{\sigma^{2}(x_{i} - \bar{x})^{2}}{S_{XX}})$，
						- 枢轴量
						  collapsed:: true
							- 同理，可以构造t枢轴量，
				- 新预测值Y0
					- 概述
					  collapsed:: true
						- 样本数据之外的给定的x0，理论上也有对应的y0，
						- 此处的x0只是不在样本点内，但一般仍落在Xi的上下界范围内，
						- 此处假定y0也为*随机变量*，其随机性也由误差εi给定，即$Y_{0} \sim N(\beta_{0} + \beta_{1}x_{0}, \sigma^{2})$，
						- 因此一般称对y0的估计量为“预测”量，
					- 点估计
						- 类似EYi，为$b_{0} + b_{1}x_{0}$，一般记为$\hat{y_{0}}$，
						- 由于b0，b1都是随机变量Y的函数，因此$\hat{y_{0}}$也是随机变量Y的函数，
						- 称$\hat{y_{0}}$为“无偏”预测，即$E(y_{0} - \hat{y_{0}}) = 0$，
					- 区间估计
						- 由于假定y0为随机变量，所以此处不使用$\hat{y_{0}}$的分布构造区间估计，
						- 设统计量$\psi = y_{0} - \hat{y_{0}}$，
						- 可以算得$\psi \sim N(0, \sigma^{2} + \dfrac{\sigma^{2}}{n} + \dfrac{\sigma^{2}(x_{i} - \bar{x})^{2}}{S_{XX}})$，
							- $\psi = y_{0} - \hat{y_{0}} = y_{0} - b_{0} - b_{1}x_{0}$，
							- y_{0}为新的随机变量，因此根据假定，其与原来的随机变量的函数$\hat{y_{0}}$独立，
							- 所以$\text{Var}(\phi) = \text{Var}(y_{0}) + \text{Var}(b_{0}) + x_{0}^{2}\text{Var}(b_{1})$，
							- $= \sigma^{2} + \dfrac{\sigma^{2}}{n} + {\bar{x}}^{2}\dfrac{\sigma^{2}}{S_{XX}} + \dfrac{\sigma^{2}}{S_{XX}}$
		- 线性相关系数R2（决定系数）
		  collapsed:: true
			- 定义
			  collapsed:: true
				- 称$R^{2} = \dfrac{SSR}{S_{YY}} = 1 - \dfrac{SSE}{S_{YY}}$为决定系数，
				- 即根据给定模型，由Xi解释的Yi的变异性的程度，
				- 由于$S_{YY} = SSR + SSE$，所以有$0 \le R^{2} \le 1$，
			- 性质
			  collapsed:: true
				- 可由Delta方法算得，$E(R^{2}) \approx \dfrac{\dfrac{\beta_{1}^{2}S_{XX}}{n - 1}}{\dfrac{\beta_{1}^{2}S_{XX}}{n - 1} + \sigma^{2}}$，
				- $R^{2} = 1 \leftrightarrow \forall j, \varepsilon_{j} = 0$，
				- $R^{2} = 0 \leftrightarrow b_{0} = \bar{y} , b_{1} = 0$，
				- R2说明的为Xi解释的Yi的变异性的程度，不能说明线性模型的适用性，
				- R2不能说明斜率的大小，
	- 模型的其他形式
	  collapsed:: true
		- X的标准化
		  collapsed:: true
			- 重新定义X的形式，将模型写为$Y_{i} = \beta_{0}' + \beta_{1}'(X_{i} - \bar{X}) + \varepsilon_{i}$，
			- $\hat{b_{0}}' = \hat{b_{0}} + \hat{b_{1}}\bar{X} = \bar{Y}$，
			- $\hat{b_{1}}' = \hat{b_{1}} = \dfrac{S_{XY}}{S_{XX}}$，
			- 新的模型与原模型得到的拟合数据$\hat{Y}$是一样的（估计值b0'和b1'也发生了变化），
			- 但新得到的b0'和b1'之间不相关，更易于进行区间估计的分析，
		- 截距为0（模型过原点）
		  collapsed:: true
			- 模型为$Y_{i} = \beta_{1}X_{i} + \varepsilon_{i}$，此时只有一个未知参数，
			- 根据最小二乘法，可解得$\hat{b_{1}} = \dfrac{\sum\limits_{i = 1}^{n}X_{i}Y_{i}}{\sum\limits_{i = 1}^{n}X_{i}^{2}}$，
			- 方差的无偏估计MSE = $\dfrac{SSE}{n - 1} = \dfrac{\sum\limits_{i = 1}^{n}Y_{i}^{2} - \hat{b_{1}}\sum\limits_{i = 1}^{n}X_{i}Y_{i}}{n - 1}$，
			  collapsed:: true
				- 由于只有一个参数需要估计，所以自由度为n - 1，
			- 模型的其他估计值的推导方式相同，
			- 无截距模型的R2计算方式为$\dfrac{\sum\limits_{i = 1}^{n}\hat{Y_{i}}^{2}}{\sum\limits_{i = 1}^{n}Y_{i}^{2}}$，
	- 二元正态模型
	  collapsed:: true
		- 模型
		  collapsed:: true
			- 回归模型仍然为$Y_{i} = \beta_{0} + \beta_{1}X_{i} + \varepsilon_{i}$，
			- 此处假设X，Y均为随机变量，且其联合分布为二元正态分布，
			- (X, Y)的具体分布未知，只假定条件分布$Y_{i}|X \sim N(\beta_{0} + \beta_{1}X, \sigma_{1.2}^{2})$，
			- 根据二元正态的性质，可知$\beta_{0} = \mu_{1} - \mu_{2}\rho\dfrac{\sigma_{1}}{\sigma_{2}}$，$\beta_{1} = \rho\dfrac{\sigma_{1}}{\sigma_{2}} = \dfrac{\sigma_{12}}{\sigma_{2}^{2}}$，
		- 参数的估计
		  collapsed:: true
			- 同样可以使用最大似然法估计回归系数，
			- $\hat{b_{0}} = \bar{Y} - \hat{b_{1}}\bar{X}$，
			- $\hat{b_{1}} = \dfrac{\sum\limits_{i = 1}^{n}X_{i}Y_{i} - n\bar{X}\bar{Y}}{\sum\limits_{i = 1}^{n}X_{i}^{2} - n\bar{X}^{2}} = \dfrac{S_{XY}}{S_{XX}}$，
			- $\hat{\rho} = \dfrac{S_{XY}}{\sqrt{S_{XX}}\sqrt{S_{YY}}}$，即样本相关系数r，
		- 相关性的检验
		  collapsed:: true
			- 由$\beta_{1} = \rho\dfrac{\sigma_{1}}{\sigma_{2}} = \dfrac{\sigma_{12}}{\sigma_{2}^{2}}$，可知X，Y之间的相关系数ρ可以用于检验X，Y之间是否有线性关系，
			- 假设$H_{0} : \rho = 0 \leftrightarrow H_{1} : \rho \ne 0$，
			- 检验统计量
			  collapsed:: true
				- 定理：H0成立时，t统计量$t = \dfrac{r{\sqrt{n - 2}}}{1 - r^{2}} \sim t(n - 2)$，可用于构造t检验，
		- @相关性的区间估计
		  collapsed:: true
			- 对假设$H_{0} : \rho = \rho_{0} \leftrightarrow H_{1} : \rho \ne \rho_{0}$的检验更为复杂，
			- 可以反转对应的拒绝域，构造相关系数的区间估计，
- 多元线性回归（MLR）
	- 模型与假定
		- 模型
			- 此处Yi受多个Xi的影响，
			- 因此模型为$Y_{i} = \beta_{0} + \beta_{1}X^{(1)}_{i} + \beta_{2}X^{(2)}_{i} +… + \beta_{k}X^{(k)}_{i} + \varepsilon_{i}$，
			  collapsed:: true
				- 函数可以不为线性函数，即$Y_{i} = \beta_{0} + \beta_{1}f_{1}(X^{(1)}_{i}) + \beta_{2}f_{2}(X^{(2)}_{i}) + \varepsilon_{i}$，
				- 预测变量Xi间存在交互作用时也可以使用模型，$Y_{i} = \beta_{0} + \beta_{1}f_{1}(X^{(1)}_{i}) + \beta_{2}f_{2}(X^{(2)}_{i}) + \beta_{3}f_{3}(X^{(1)}_{i}, X^{(2)}_{i}) + \varepsilon_{i}$，
			- 称此处的参数βi为偏回归系数，
			- 一般写为矩阵形式，即$\vec{Y}_{n*1} = \mathbf{X}_{n*(k+1)}\vec{\beta} + \vec{\varepsilon}$，
				- 应注意此处X为n行（n组数据）*k+1列*（包括常数项β0，所以X的*第一列全为1*）矩阵，
				- ![image.png](../assets/image_1673917632526_0.png){:height 111, :width 326}，
				- 方程组$\begin{cases} \beta_{0} + \beta_{1}X^{(1)}_{1} + \beta_{2}X^{(2)}_{1} +… + \beta_{k}X^{(k)}_{1} + \varepsilon_{1} = Y_{1}\\ \beta_{0} + \beta_{1}X^{(1)}_{2} + \beta_{2}X^{(2)}_{2} +… + \beta_{k}X^{(k)}_{2} + \varepsilon_{2} = Y_{2}\\ \vdots \\ \beta_{0} + \beta_{1}X^{(1)}_{n} + \beta_{2}X^{(2)}_{n} +… + \beta_{k}X^{(k)}_{n} + \varepsilon_{n}  = Y_{n}\\ \end{cases}$，
		- 统计假定
		  collapsed:: true
			- 类似一元情况，
			- 一般将X看成常量，
			  collapsed:: true
				- @将X看成随机变量时，仍可根据给定X时Y的条件分布做出推断，
			- εi为随机变量，且两两不相关；一般记$\varepsilon_{i} \sim N(0, \sigma^{2})$（Gauss-Markov条件），
			- 可知*每个*随机*变量*$Y_{i} \sim N(x_{i}^{T}\beta, \sigma^{2})$，而随机*向量*Y的分布则是$N_{n}(X\beta, \sigma^{2}I_{n})$，
		- @标准化模型
	- 模型的统计推断
		- 参数估计
			- 回归系数的估计
			  collapsed:: true
				- 最小二乘法
				  collapsed:: true
					- 类似一元情况，
					- 此处的函数为k元函数，即$Q(b_{0}, b_{1}, \cdots, b_{k}) = \sum\limits_{i = 1}^{n}(Y_{i} - b_{0} - \sum\limits_{j = 1}^{k}b_{j}X^{(j)}_{i})^{2}$，此处Xi，Yi为确定的样本值 ，
					- 求偏导数，并令偏导数为0，可得k + 1个方程，
					- （标量形式）
					  collapsed:: true
						- ![image.png](../assets/image_1673595392506_0.png)
						- 形式较为繁琐，一般将其写为矩阵形式再进行分析，
				- 残差平方和
				  collapsed:: true
					- 类似一元情况，希望最小化残差e，
					- 设$X\beta + e = \hat{y}$，即$e = \hat{y} -X\beta$，
					- 所以残差平方和$\sum\limits_{i=1}^{n}e_{i}^{2} = e^{T}e = (y - X\beta)^{T}(y - X\beta) = S(\beta)$，
				- 方程的化简
				  collapsed:: true
					- 转置为*线性*运算：$(y - X\beta)^{T} = y^{T} - (X\beta)^{T}$，
					- 方程可写为$y^{T}y - \beta^{T}X^{T}y - y^{T}X\beta + \beta^{T}X^{T}X\beta$，
					- 其中$\beta^{T}X^{T}y，y^{T}X\beta$都是标量，所以可以合并，
				- 求解（矩阵求导）
				  collapsed:: true
					- $\dfrac{\partial S} {\partial \beta} = -2X^{T}y + 2X^{T}X\beta$，
					  collapsed:: true
						- 此处β为求导变量；x，y均为常量，
						- $\beta^{T}X^{T}y$为βi的线性组合（一次函数），
						- $\beta^{T}X^{T}X\beta$为βi的二次型（二次函数），
						- [[多变量微积分]]
					- 令其为0，可得$X^{T}X\hat{\beta} = X^{T}y$，
					- 若$X^{T}X$可逆，则可解得$\hat{\beta} = (X^{T}X)^{-1}X^{T}y$，
					  collapsed:: true
						- 若X列满秩（必要条件为k+1 \le n），则矩阵$X^{T}X$可逆，
				- （线性代数含义）
				- 极大似然估计
				  collapsed:: true
					- 在正态分布假定下，也可使用极大似然估计求解参数，
					- $f_{Y}(y_{i})=\frac{1}{\sqrt{2 \pi} \sigma} \exp{(-\frac{(Y_{i} - x_{i}^{T}\beta)^{2}}{2 \sigma^{2}})}$，
					- $L(\beta) = (2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}\exp[{-\frac{(y - X\beta)^{T}(y - X\beta)}{2 \sigma^{2}}}]$，
					- 可见，对于正态分布假定，多元情况的极大似然估计值同样与最小二乘方法算出的值相同，
					- 方差的估计量$\hat{\sigma^{2}} = \dfrac{(y - X\hat{\beta})^{T}(y - X\hat{\beta})}{n}$，为有偏估计，
				- 估计量的性质
				  collapsed:: true
					- 无偏性：$E(\hat{\beta}) = \beta$，
					- 协差阵：$\text{Cov}(\hat{\beta}) = \sigma^{2}(X^{T}X)^{-1}$，
					  collapsed:: true
						- 计算
						  collapsed:: true
							- $\text{Cov}(\hat{\beta}) = ((X^{T}X)^{-1}X^{T}y) = [(X^{T}X)^{-1}X^{T}]\text{Cov}(y)[((X^{T}X)^{-1}X^{T}]^{T}$，
							- 根据统计假定，$\text{Cov}(y) = \sigma^{2}I_{n}$，
							- 可化简得到上式，
						- 一般设预测矩阵$C = (X^{T}X)^{-1}$，C的元素为cij，
						  collapsed:: true
							- cjj \cdot \sigma^{2}为每个估计量bj的方差，
							- cij \cdot \sigma^{2}为估计量bi和bj之间的协方差，
					- Gauss最小二乘定理
					  collapsed:: true
						- 若预测变量矩阵C满秩，
						- 则对任意r + 1为常数向量c，参数β的线性组合$c^{T}{\beta}$的所有无偏估计值中，基于最小二乘估计的$c^{T}\hat{\beta}$方差最小，
						- 称$c^{T}\hat{\beta}$为最佳线性无偏估计量（BLUE），
			- 方差的估计
			  collapsed:: true
				- 残差$e = y - \hat{y} = (I - H)y = y - X\hat{\beta}$，
				- 残差的性质
				  collapsed:: true
					- $E(\vec{e}) = 0$，
					  collapsed:: true
						- $E[(I - H)y] = (I - H)E(y) = (I - H)X\beta$，
						- $= X\beta - X(X^{T}X)^{-1}X^{T}X\beta = 0$，
					- $\text{Cov}(\vec{e}) = \sigma^{2}(I - H)$，
					  collapsed:: true
						- $=(I - H)\text{Cov}(y)(I - H)^{T} = \sigma^{2}(I - H)(I - H)^{T} = \sigma^{2}(I - H)$，
				- SSE的估计
				  collapsed:: true
					- 残差平方和$e^{T}e = (y - X\hat{\beta})^{T}(y - X\hat{\beta})$，
					- 对于估计量$\hat{\beta} = (X^{T}X)^{-1}X^{T}y$，
					- 可算得$SSE = y^{T}y - 2\hat{\beta}^{T}X^{T}y + \hat{\beta}^{T}X^{T}X \hat{\beta} = y^{T}y - \hat{\beta}^{T}X^{T}y$，
					- 也可写为$y^{T}(I - H)y$，
				- MSE的估计
				  collapsed:: true
					- @MSE = $\dfrac{SSE}{n - k - 1}$，为方差σ2的无偏估计，
					  collapsed:: true
						- $E(\hat{\sigma^{2}}) = \dfrac{1}{n - k - 1}E[y^{T}(I-H)y]$，
						- 利用迹的可交换性，可以简化计算，
						- 即$E[y^{T}(I-H)y] = E(\text{tr}[y^{T}(I-H)y]) = E(\text{tr}[(I-H)yy^{T}])$，
						- $= \text{tr}[(I-H)E(yy^{T})]$；期望的性质，
						- 其中，$E(yy^{T}) = \text{Cov}(y) + E(y)E(y^{T})= \sigma^{2}I_{n} + X\beta\beta^{T}X^{T}$；协差阵的定义，
						  id:: 63c608cc-0e22-498f-af4c-279c13e0037b
						- 而$\text{tr}(I-H) = \text{tr}(I) - \text{tr}(H) = n - \text{tr}(X(X^{T}X)^{-1}X^{T})$
					- 由于模型中有k+1个参数需要估计，所以此处自由度为n - k - 1，
			- 拟合值的估计
			  collapsed:: true
				- 由于$\hat{\beta} = (X^{T}X)^{-1}X^{T}y$，
				- 所以对于样本矩阵X，拟合值向量$\hat{y} = X\hat{\beta} = X \cdot (X^{T}X)^{-1}X^{T}y = Hy$，
				  collapsed:: true
					- 应注意此处X为n行(k+1)列矩阵，并非方阵；所以求逆运算不能拆分，
			- 帽子矩阵
				- 一般称矩阵$H_{n*n} = X(X^{T}X)^{-1}X^{T}$为帽子矩阵，
				- 可见帽子矩阵为$y \to \hat{y}$的一个线性变换矩阵，
				- 性质
				- 投影矩阵：$H = H^{T} = H^{2}$，
				  collapsed:: true
					- 根据投影矩阵的性质，I - H也为投影矩阵，
				- $X^{T}(I - H) = 0$，
		- 估计量的分布
		  collapsed:: true
			- 条件：正态性假定$Y_{n} \sim N_{n}(X\beta, \sigma^{2}I_{n})$，观测矩阵X为列满秩，
			- 参数向量$\hat{\beta}_{k+1} \sim N_{k+1}(\beta, \sigma^{2}(X^{T}X)^{-1})$，
			  collapsed:: true
				- $\hat{\beta} = (X^{T}X)^{-1}X^{T}y$，
			- 每个分量βj的分布为$\hat{\beta}_{j} \sim N(\beta_{j}, \sigma^{2}C_{jj})$，其中C_{jj}为矩阵$(X^{T}X)$中的元素，
			- 估计量：在给定的向量x0处，$\hat{y_{0}} \sim N(x_{0}^{T}{\beta}, \sigma^{2}x_{0}^{T}(X^{T}X)^{-1}x_{0})$，
			- 残差$e_{n} \sim N_{n}(0, \sigma^{2}(I - H))$，
			  collapsed:: true
				- 残差$e = y - \hat{y} = (I - H)y = y - X\hat{\beta}$，
			- 残差平方和：$e^{T}e = n\hat{\sigma}^{2} \sim \sigma^{2}\chi^{2}_{n-k-1}$，且与参数向量$\hat{\beta}_{k+1}$独立，
			  collapsed:: true
				- （需要先证明残差，参数组合成的向量也为正态分布）
				- $\text{Cov}(\hat{\beta}, e) = \text{Cov}[(X^{T}X)^{-1}X^{T}y, (I - H)y]$，
				- $=(X^{T}X)^{-1}X^{T}\cdot\text{Cov}(y,y)\cdot(I - H) = 0$，
		- 假设检验
		  collapsed:: true
			- 方差的分解
			  collapsed:: true
				- 均值总平方和$S_{YY} = \sum\limits_{j=1}^{n}(y_{j} - \bar{y})^{2} = y^{T}y - n\bar{y}^{2}$，
				- 残差平方和$SSE = e^{T}e =  y^{T}y - \hat{\beta}^{T}X^{T}y$，
				- 两者相减，可得回归平方和：$SSR = SYY - SSE = \hat{\beta}^{T}X^{T}y - n\bar{y}^{2}$，
				  collapsed:: true
					- 也可根据定义，$SSR = \sum\limits_{j=1}^{n}(\hat{y_{j}} - \bar{y})^{2} = \hat{y}^{T}\hat{y} - n\bar{y}^{2}$，
					- 由于$\hat{y} = X\hat{\beta} = X \cdot (X^{T}X)^{-1}X^{T}y = Hy$，
					- 所以$\hat{y}^{T}\hat{y} = y^{T}H^{T}Hy = (Hy)^{T}y = (X\hat{\beta})^{T}y = \hat{\beta}^{T}X^{T}y$；投影矩阵的性质，
			- 回归显著性检验
			  collapsed:: true
				- 假设
				  collapsed:: true
					- 对于多元模型，一个简单的检验为$H_{0} : \beta_{0} = \beta_{1} = … = \beta_{k} = 0 \leftrightarrow H_{1} : \beta_{i} \ne 0$，
				- 检验统计量
				  collapsed:: true
					- @统计量$V = (X^{T}X)^{\frac{1}{2}}(\hat{\beta} - \beta)$，则$\dfrac{V^{T}V}{\sigma^{2}} \sim \chi^{2}_{k+1}$，
					- 由于$e^{T}e = n\hat{\sigma}^{2} \sim \sigma^{2}\chi^{2}_{n-k-1}$，且与参数向量$\hat{\beta}_{k+1}$独立，
					- 可构造F统计量$F = \dfrac{V^{T}V/(k+1)}{SSE/(n-k-1)} \sim F(k+1, n-k-1)$，
				- R2的计算
				  collapsed:: true
					- 定义决定系数$R^{2} = 1 - \dfrac{SSE}{S_{YY}} = \dfrac{SSR}{S_{YY}}$，
			- 单个参数的检验（边际检验）
			  collapsed:: true
				- 假设：$H_{0} : \beta_{j} = 0 \leftrightarrow H_{1} : \beta_{j} \ne 0$，
				- 检验统计量
				  collapsed:: true
					- @t统计量$t_{0} = \dfrac{\hat{\beta_{j}}}{\sqrt{\hat{\sigma^{2}}C_{jj}}}$，其中C_{jj}为矩阵$(X^{T}X)$中的元素，
					  collapsed:: true
						- $\hat{\beta_{j}} = s_{j}\hat{\beta} s_{j}^T$，其中sj为第j个元素为1，其它元素均为0的向量，
						- 根据正态分布的性质，可知$\hat{\beta_{j}}  \sim N(s_{j}^{T}\vec{\mu}, s_{j}^{T}\sigma^{2}(X^{T}X)^{-1} s_{j})$，
						- 即$\hat{\beta_{j}}  \sim N(\beta_{j}, \sigma^{2}C_{jj})$，
					- 一般记${\sqrt{\hat{\sigma^{2}}C_{jj}}}$为se(βj)，即回归系数βj的标准误差，
					  id:: 63c50891-abde-4d7c-a4f6-f721d883b60f
					- 拒绝域为$|t_{0}| > t_{n - k -1}{(\frac{\alpha}{2})}$，
			- 多个参数的检验（附加平方和方法）
			  collapsed:: true
				- 假设
				  collapsed:: true
					- 将参数向量分块，$\vec{\beta} = \begin{pmatrix} \beta1_{p-r} \\ \beta2_{r} \end{pmatrix}$，
					- 假设：$H_{0} : \beta_{2} = 0 \leftrightarrow H_{1} : \beta_{2} \ne 0$，
				- 似然比检验
				  collapsed:: true
					- ### 模型拆分
					- 将模型拆分为$y = X_{1}\beta_{1} + X_{2}\beta_{2} + \varepsilon$，
					- H0成立的情况下，模型为$y = X_{1}\beta_{1}  + \varepsilon$，
					- ### 参数的估计
					- 根据β的估计量$\hat{\beta} = (X^{T}X)^{-1}X^{T}y$，
					- 可写出$\hat{\beta_{1}} = (X_{1}^{T}X_{1})^{-1}X_{1}^{T}y$，
					- ### 方差的估计
					- 额外平方和：SSE1 - SSE = $(y - X_{1}\hat{\beta}_{1})^{T}(y - X_{1}\hat{\beta}_{1}) - (y - X\hat{\beta})^{T}(y - X\hat{\beta})$，
					  collapsed:: true
						- 由于原模型使用了更多的参数，所以理论上原模型的残差SSE应该更小，而回归平方和SSR更大，
					- $SSR_{1} = \hat{\beta_{1}} X_{1}^{T}y$，自由度为p - r，
					- ### F统计量的构造
					- $\dfrac{(SSE1 - SSE) / r}{s^{2}} \sim F_{r, n-r-1}$，
				- @偏F检验
				  collapsed:: true
					- 给定β1后，β2的回归平方和为$SSR_{2|1} = SSR - SSR_{1}$（一般*不等于*SSR2），自由度为p - (p - r) = r；称为“附加平方和”，
					  collapsed:: true
						- 将给定β1后，β2的回归平方和记为$SSR_{2|1}$，
						- 将给定β1，β2后，β0的回归平方和记为$SSR_{0|1, 2}$，
						- 将给定β0后，β1和β2的（总）回归平方和记为$SSR_{1, 2|0}$，
					- 可以证明，在H0成立的情况下，$SSR_{2|1}$与MSE独立，且统计量$F = \dfrac{SSR_{2|1} / r}{MSE}$为非中心F分布，
				- 特例：X中的列为正交列
				  collapsed:: true
					- 可以证明，在两组预测变量X1中的列与X2中的列正交（即X1与X2不相关）时，偏F检验的功效最大；
					- 将检验方程写为分块矩阵，即${\begin{pmatrix} X_{1}^{T}X_{1} & X_{1}^{T}X_{2} \\ X_{2}^{T}X_{1} & X_{2}^{T}X_{2}  \\ \end{pmatrix}}\begin{pmatrix} \beta1_{p-r} \\ \beta2_{r} \end{pmatrix} = \begin{pmatrix} X_{1}^{T}y \\ X_{2}^{T}y \end{pmatrix}$
					- 根据假设，可知$X_{1}^{T}X_{2} =  X_{2}^{T}X_{1} = 0$，
					- 所以按照分块矩阵乘法，可知$\hat{\beta_{1}} = (X_{1}^{T}X_{1})^{-1}X_{1}^{T}y$，
					  $\hat{\beta_{2}} = (X_{2}^{T}X_{2})^{-1}X_{2}^{T}y$，且两者无关，
			- @一般线性假设的检验
			  collapsed:: true
				- 假设
				  collapsed:: true
					- $H_{0} : C\beta = 0 \leftrightarrow H_{1} : C\beta \ne 0$，其中C为m行p列常数矩阵，秩为r，
				- 检验统计量
				  collapsed:: true
					- 根据正态分布的性质，$C\hat{\beta} \sim N_{r}(C\beta, \sigma^{2}C(X^{T}X)^{-1}C^{T})$，
					- @可构造$\chi^{2}$统计量，即$\dfrac{(C\hat{\beta})^{T}(C(X^{T}X)^{-1}C^{T})^{-1}(C\hat{\beta})}{\sigma^{2}}$，
		- 置信区间
		  collapsed:: true
			- 单个参数的区间估计
			  collapsed:: true
				- 根据单参数的t检验，可知每个参数的1 - α区间为$\hat{\beta_{j}} - t_{n - p}{\sqrt{\hat{\sigma^{2}}C_{jj}}} \le \beta_{j} \le \hat{\beta_{j}} + t_{n - p}{\sqrt{\hat{\sigma^{2}}C_{jj}}}$，
			- 多个参数的联合区间
			  collapsed:: true
				- 联合分布
				  collapsed:: true
					- @可以证明，$\dfrac{(\hat{\beta} - \beta)^{T}X^{T}X(\hat{\beta} - \beta) / p}{MSE} \sim F_{p, n-p}$，
					- 因此可构造p维向量β的一个联合置信域，
				- Bonferroni方法
				  collapsed:: true
					- 根据Bonferroni不等式，将每个估计量的置信系数增加，即可确保多个系数的联合置信区间的置信系数仍为1 - α，
				- @Scheffe多重比较
			- EY的区间估计
			  collapsed:: true
				- 设x0为某一预测变量*向量*，则$E(Y_{0}|x_{0}) = x_{0}^{T}{\beta}$，即为参数β的函数，
				- 一个对应的估计为$\hat{y_{0}} = x_{0}^{T}\hat{\beta}$；
				  collapsed:: true
					- 根据Gauss最小二乘定理，可知$\hat{y_{0}} = x_{0}^{T}\hat{\beta}$为一个最佳线性无偏估计量，
				- 在正态假定下，可知$\hat{y_{0}} \sim N(x_{0}^{T}{\beta}, \sigma^{2}x_{0}^{T}(X^{T}X)^{-1}x_{0})$，
				- 所以有1 - α区间$\hat{y_{0}} - t_{n - k-1}(\frac{\alpha}{2})\sqrt{\hat{\sigma^{2}}x_{0}^{T}(X^{T}X)^{-1}x_{0}} \le y_{0} \le \hat{y_{0}} + t_{n - k-1}(\frac{\alpha}{2})\sqrt{\hat{\sigma^{2}}x_{0}^{T}(X^{T}X)^{-1}x_{0}}$，
			- 新观测值的区间估计
			  collapsed:: true
				- 类似一元情况，此处的Y0也为随机变量，
				- 新观测值$Y_{0} = x_{0}^{T}\beta + \varepsilon_{0}$，对应的估计为$\hat{y_{0}} = x_{0}^{T}\hat{\beta}$，
				- 设统计量$\psi = y_{0} - \hat{y_{0}}$，也有$\psi \sim N(0, \sigma^{2} + \sigma^{2}x_{0}^{T}(X^{T}X)^{-1}x_{0})$，
				- 1 - α区间为$\hat{y_{0}} - t_{n - k-1}\sqrt{\hat{\sigma^{2}} + \hat{\sigma^{2}}x_{0}^{T}(X^{T}X)^{-1}x_{0}} \le y_{0} \le \hat{y_{0}} + t_{n - k-1}\sqrt{\hat{\sigma^{2}} + \hat{\sigma^{2}}x_{0}^{T}(X^{T}X)^{-1}x_{0}}$，
	- 多元模型的外推
	  collapsed:: true
		- 不同于一元模型；多元模型中预测变量的分布集合一般不是完整的矩形，如 ![image.png](../assets/image_1673858161162_0.png){:height 150, :width 173}，
		- 因此多元模型更应注意避免数据的外推导致的错误，
		- 分布椭球的分析
		- 设帽子矩阵$X(X^{T}X)^{-1}X^{T}$的对角线元素为h，
		- 可以证明，椭球的点x的集合应满足$x^{T}(X^{T}X)^{-1}x \le \max h$，
		- 即如果某一预测变量（向量）xk使得$x_{k}^{T}(X^{T}X)^{-1}x_{k} > \max h$，则在该向量处模型可能并不适用，
-
- 方差分析（ANOVA）
  collapsed:: true
	- 概念
	  collapsed:: true
		- 将响应变量Y的方差“分解”，并分析这些方差是否可以由X的方差的变化解释，
	- 方差的分解
	  collapsed:: true
		- $S_{YY} = \sum\limits_{i = 1}^{n}(Y_{i} - \bar{Y})^{2} = \sum\limits_{i = 1}^{n}[(Y_{i} - \hat{Y_{i}}) + (\hat{Y_{i}} - \bar{Y})]^{2}$，
		- $= \sum\limits_{i = 1}^{n}(\hat{Y_{i}} - \bar{Y})^{2} + \sum\limits_{i = 1}^{n}(Y_{i} - \hat{Y_{i}})^{2}$，
		  collapsed:: true
			- 其中后半部分为SSE（$\sum \limits _{i= 1}^{n}(Y_{i} - \hat{Y_{i}})^{2} = \sum\limits_{i=1}^{n}e_{i}^{2}$），即残差平方和，
			- 称前半部分$\sum\limits_{i = 1}^{n}(\hat{Y_{i}} - \bar{Y})^{2}$为SSR，即回归平方和，
		- 因此有SYY = SSR + SSE，
		- 由于SSE = $S_{YY} - b_{1}S_{XY}$，所以有SSR = b1SXY，
	- 方差的分布
	  collapsed:: true
		- 自由度
		  collapsed:: true
			- SYY只受$\bar{Y}$的影响，因此自由度为n - 1，
			- SSE中有两个估计量b0，b1，因此自由度为n - 2，
			- SSR由估计量b1完全确定，因此自由度为1，
		- 定理
		  collapsed:: true
			- $SSE = \dfrac{(n - 2)MSE}{\sigma^{2}} \sim \chi^{2}(n - 2)$，
			- $\dfrac{SSR}{\sigma^{2}} \sim \chi^{2}(1)$，
	- 统计量
	  collapsed:: true
		- 可构造F统计量$F = \dfrac{SSR/1}{SSE/(n-2)} \sim F(1, n-2)$，
		- F统计量与检验斜率β1的t统计量的平方相同，
	- 单因子方差分析
	  collapsed:: true
		- 构造回归模型
			- 设计矩阵
			  collapsed:: true
				- 设随机变量$Z_{i} = \begin{cases} 1 & 观测值来自总体i \\ 0 & 其它 \end{cases}$，
				- 可以构造线性模型$Y_{i} = \beta_{0} + \beta_{1}Z^{(1)}_{i} + \beta_{2}Z^{(2)}_{i} + \varepsilon_{i}$，
				- 此处设$\beta_{0} = \mu，\beta_{1} = \tau_{1}，\beta_{2} = \tau_{2}$，
				- ![image.png](../assets/image_1673918236701_0.png){:height 194, :width 291}，
				-
	- 多因子方差分析
	  collapsed:: true
		- 相同样本两因子方差分析
		- 不同样本两因子方差分析
		- 协方差分析
		- 随机实验方差分析
		- 多因子方差分析
		- 交互模型分析
	- 多重比较
-
- 模型适用性检验
	- 统计假设
	  collapsed:: true
		- 线性关系，
		- Gauss-Markov条件：εi两两不相关；且$\varepsilon_{i} \sim N(0, \sigma^{2})$，
	- 残差诊断
		- 概述
		  collapsed:: true
			- 在线性模型的条件下，基本的统计假定都是关于误差变量 ε 的性质，
			- 由于残差统计量$e = y - \hat{y}$可以看作对误差的拟合；因此，若误差不符合基本假设，则应该能够从残差上得到一定体现，
			- 然而残差之间并不独立；当需要估计的参数数量为p个时，残差的自由度为n - p；一般认为，n较大而p较小时可以忽略残差不独立的影响，
		- 残差的尺度化
		  collapsed:: true
			- 标准化
			  collapsed:: true
				- $d_{i} = \dfrac{e_{i}}{\sqrt{MSE}}$；
				- 理论上，di应有0均值和近似相同的方差；
				- 若某个di方差较大，则说明可能有离群点，
			- 学生化
			  collapsed:: true
				- 由于残差之间并不独立，即$\text{Cov}(\vec{e}) = \sigma^{2}(I - H)$中的I - H一般不是对角矩阵，所以标准化公式不能准确地计算残差，
				- 记H的对角线元素为hij，则$\text{Var}(e_{i}) = \sigma^{2}(1 - h_{ii})$，$\text{Cov}(e_{i}, e_{j}) = -\sigma^{2}h_{ij}$，
				- 定义学生化残差$r_{i} = \dfrac{e_{i}}{\sqrt{MSE(1 - h_{ii})}}$，
				- 一般而言，靠近Xi的分布空间的中心的点$x_{i} - \bar{x}$较小，其对应的残差ei的方差较大，
				- 例如，对于一元回归模型，$r_{i} = \dfrac{e_{i}}{\sqrt{MSE\cdot (1 - \dfrac{1}{n} - \dfrac{(x_{i} - \bar{x})^{2}}{S_{XX}})}}$，
				- 然而，违背假设的点更可能出现在分布空间的边缘处，所以较难以通过检查残差的值来判断；尤其对于大型数据集，
			- R-学生残差
			  collapsed:: true
				- 定义$S^{2}_{(i)}$为根据在删除第i个样本点后得到的模型计算得到的MSE，
				- 可以证明$S^{2}_{(i)} = \dfrac{(n-p)MSE - \dfrac{e_{i}^{2}}{1 - h_{ii}}}{n-p-1}$，
				- 称用$S^{2}_{(i)}$代替MSE计算得到的为R-学生残差，
				- 若第i个观测值为强影响点，则两者可能存在较明显的差异，
			- PRESS统计量
			  collapsed:: true
				- 定义预测误差$e_{(i)} = y_{i} - \hat{y}_{(i)}$，
				  collapsed:: true
					- 其中，$\hat{y}_{(i)}$为在删除第i个样本点后得到的模型中带入Xi，拟合得到的Yi的值，
					- 也称为PRESS残差，
					- 可以证明$e_{(i)} = \dfrac{e_{i}}{{1 - h_{ii}}}$，$\text{Var}(e_{i}) = \dfrac{\sigma^{2}}{1 - h_{ii}}$，
				- 标准化的PRESS残差为$\dfrac{e_{i}}{\sqrt{\sigma^{2}(1 - h_{ii})}}$；
				  collapsed:: true
					- 若用MSE估计σ2，则该变量等同于学生化残差，
				- 定义PRESS残差的平方和为PRESS统计量，即$\sum\limits_{i=1}^{n}(\dfrac{e_{i}}{{1 - h_{ii}}})^{2}$，
				- 一般而言，PRESS统计量较小的模型更为优良，
				- 定义预测$R^{2} = 1 -\dfrac{PRESS}{S_{YY}}$，其性质与作用类似R2统计量，
		- 残差图
		  collapsed:: true
			- 正态概率图
			  collapsed:: true
				- 一般样本量较大（≥20）时，得到的图形较为稳定，
				- 示例（此处残差在横轴，若残差在纵轴则需要反转），
				- ![image.png](../assets/image_1673944885280_0.png)
			- 残差与拟合值图
			  collapsed:: true
				- 一般根据ei（或学生化残差ri）和拟合值$\hat{y}_{i}$作图，因为两者不相关；而ei和观测值${y}_{i}$是相关的，
				- ### 理想情况（方差齐性）
				- ![image.png](../assets/image_1673945142419_0.png){:height 136, :width 140}，
				- ### 方差不齐的情况
				- ![image.png](../assets/image_1673945177563_0.png){:height 113, :width 292}，
				- 一般通过对模型进行变换，来改善方差不齐的情况，
				- ### 与预测值相关的情况
				- ![image.png](../assets/image_1674176607357_0.png){:height 95, :width 182}，
				- 数值计算不正确，或者忽略了β0，
			- （时间序列图与自相关）
		- 杠杆率
			- 称帽子矩阵H的对角线元素hjj为杠杆率，
			- @一元情况下，可以算得$h_{jj} = \dfrac{1}{n} + \dfrac{(x_{j} - \bar{x})^{2}}{\sum\limits_{i=1}^{n}(x_{i} - \bar{x})^{2}}$，
			- 杠杆率可以用于解释变量空间中第j个数据点到其它n - 1个数据点距离远近的度量，
			- $\hat{y}_{j} = h_{jj}y_{j} + \sum\limits_{k \ne j}h_{jk}y_{k}$，
		- @偏回归图与偏残差图
	- 模型的失拟
	  collapsed:: true
		- 指所选择的模型并不适合具体的数据，
		- ### 消除观测误差
		- 一种测量失拟的方式为，在每个数据点重复进行多次实验得到多个y值，以度量测量过程产生的误差，
		- 设共有m组数据，在第i个点xi处进行了ni次观测，则总共的数据为$n = \sum\limits_{i=1}^{m} n_{i}$个，
		- ### 用近邻点代替重复实验
		- 一般情况下，多次重复实验并不可行；可以使用与xi值近邻的xj的值来替代关于xi的多次重复实验，以获得纯误差的取值，
		- 一种方式为计算“加权平方和距离”$D_{ii'}^{2}$，若D值较小，则说明两个点近邻，
		- 然后，再使用这两个点的残差（的绝对值）来估计纯误差，一般还需要一定的加权，
		- ### 方差的分解
		- 同样采用分解方差的方式来判断，将总残差平方和SSE分解为两部分，SS误差和SS失拟，
		- 即$y_{ij} - \hat{y}_{i} = (y_{ij} - \bar{y}_{i}) + (\bar{y}_{i} - \hat{y}_{i})$，
		  collapsed:: true
			- $\bar{y}_{i}$为xi处的ni次观测的平均值，
			- $\hat{y}_{i}$根据所有观测值拟合出来的模型对应的xi处的拟合值，
		- 对于所有模型，则有$\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n_{i}}(y_{ij} - \hat{y}_{i})^{2} = \sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n_{i}}(y_{ij} - \bar{y}_{i})^{2} + \sum\limits_{i=1}^{m}(\bar{y}_{i} - \hat{y}_{i})^{2}$，
		- ### @统计量的构造
		- P110
		-
	- 其它
	  collapsed:: true
		- 正态性检验
		- F检验
- 模型的诊断
	- 概述
	  collapsed:: true
		- ![image.png](../assets/image_1673941676327_0.png)
	- 杠杆点
	- 离群点
		- 概述
		  collapsed:: true
			- 与其它部分差异较大的观测值，不同的取值带来的影响也不同，
			- 离群点可能是错误的观测值，也可能是由于问题较为复杂，或者模型并不适用，
			- 若没有准确证据表明离群点是错误的，一般不应为了保证模型的拟合程度舍弃离群点，
	- 强影响点
	  collapsed:: true
		- DFFITS
		- Cook距离
		- DFBETAS
	- 多重共线性
	  collapsed:: true
		- 概念
		  collapsed:: true
			- 预测变量Xj之间存在线性关系，
			- 即X可能不为列满秩，此时预测矩阵$C = X^{T}X$为奇异矩阵，
				- 实际数据往往较为复杂，所以一般预测矩阵只是近似于奇异矩阵，
				- 表现为行列式的取值很小，如0.01，
			- 因此回归系数的计算也会变得复杂，
		- 衡量方法
		  collapsed:: true
			- 方差膨胀因子（VIF）
				- 称$(X^{T}X)^{-1}$的主对角线元素为方差膨胀因子，
				- 如果矩阵近似于奇异矩阵，则其逆矩阵的对角线元素一般较大，
- 模型的补救措施
	- 模型的修正
	  collapsed:: true
		- 概述
		  collapsed:: true
			- 当发现统计假设不能满足时，可以进行一定的数据变换；得到的结果可能会更接近于假设的条件，
		- 方差变换
		  collapsed:: true
			- 若Y的具体分布的方差与均值有关，则Y的方差一般不为确定常数，
			- 一种方式为Δ方法，即$\text{Var}[g(Y)] \approx {\sigma}^{2}{[g'(\theta)]}^{2}$；再根据对Y的分布的经验， 选择具体的变换函数，
			  collapsed:: true
				- 示例 ![image.png](../assets/image_1674118503278_0.png)，
			- 然而，对新的预测值$\hat{g(Y)}$进行反向变换得到的预测值$\hat{Y}$可能会有一定的误差，
		- 模型线性变换
		  collapsed:: true
			- 一些回归关系可能不是线性的，但部分模型可以使用合适的变换来转换为线性模型，
			  collapsed:: true
				- ![image.png](../assets/image_1674118916401_0.png)
			- 类似方差变换，变换后的（线性模型）数据一般具有一定的优良性，但转换得到的原数据可能不再有优良性，
		- 响应变量Y的变换（Box-Cox变换）
		- 最小二乘法的修正
	- 岭回归
	- 局部加权回归
	- 其它
		- 带有随机效应的模型
		- 回归树
		- 加性模型
		  collapsed:: true
			- 设$\hat{y_{n}}$为基于标准模型的预测值，而$\hat{y_{n}}^{*}$为基于其它*n - 1个*数据点的响应变量的预测值，
			- 定义差值$\delta = y_{n} - \hat{y_{n}}^{*}$；若δ较大，则第n个点为一个强影响点，
			- $\hat{y_{n}} = \hat{y_{n}}^{*} + h_{nn}\delta$，
			- $= \hat{y_{n}}^{*} + h_{nn}(y_{n} - \hat{y_{n}}^{*}) = h_{nn}y_{n} + (1 - h_{nn})\hat{y_{n}}^{*}$，
			- 其中hnn为帽子矩阵的第n个对角元素，$h_{nn} = \dfrac{1}{n} + (\dfrac{n-1}{n})^{2}\cdot \dfrac{(x_{n} - \bar{x}^{*})^{2}}{S_{XX}}$，
			- 若xn离其它观测值的均值较远，则$\hat{y_{n}}$将逐渐接近$y_{n}$，
		- 稳健回归
		- 自助法（Bootstrap）
-
- 多项式回归
- 非线性回归
  collapsed:: true
	- 模型的变换
	- 逻辑斯蒂回归
	- 泊松回归
	- 广义线性模型
-
- 其它
	- 时间序列回归
	- 实验设计
	  collapsed:: true
		- 巢式设计
		- 重复设计和相关性设计
		- 拉丁方设计
		- 响应面分析法
-
- [[多元统计]]
- [[矩阵论]]
- [[数理统计]]
- [[概率论]]
- [[线性方程组]]
-
- [[Science]]